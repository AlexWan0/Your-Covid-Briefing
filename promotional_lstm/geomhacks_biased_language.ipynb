{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geomhacks - biased language",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51aee86b3f764d158dea827bbcb2eac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28ac25f70788475893fac01a0dffe496",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0a4879e1b93d49d9b1f7ae9617629d26",
              "IPY_MODEL_a68f8f0e981d47ba84e3b7a29ca21aa8"
            ]
          }
        },
        "28ac25f70788475893fac01a0dffe496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a4879e1b93d49d9b1f7ae9617629d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c9ea777de204612ad891261d7af8453",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10824,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10824,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67fbdb87c4214e949e6026416eb8d54a"
          }
        },
        "a68f8f0e981d47ba84e3b7a29ca21aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc1bc3a46a7e45aabd84da2ab62a93ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10824/10824 [02:11&lt;00:00, 82.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_270bc8f728e14353ac4fe8647eb003c4"
          }
        },
        "9c9ea777de204612ad891261d7af8453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67fbdb87c4214e949e6026416eb8d54a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc1bc3a46a7e45aabd84da2ab62a93ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "270bc8f728e14353ac4fe8647eb003c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmtCMYrtKcV6",
        "colab_type": "code",
        "outputId": "4775eef5-5433-49ce-92a8-0693b09d574b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE-aQlg7KgrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/gdrive/My Drive/geomhacks_bias/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxtRttWGNbqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def split_3(df, sp1=0.7, sp2=0.8, shuffle=True):\n",
        "  if shuffle:\n",
        "    df = df.sample(frac=1)\n",
        "  l_sp1 = int(sp1 * len(df))\n",
        "  l_sp2 = int(sp2 * len(df))\n",
        "  print('%d %d %d' % (l_sp1, l_sp2 - l_sp1, len(df) - l_sp2))\n",
        "  return df[:l_sp1], df[l_sp1:l_sp2], df[l_sp2:]\n",
        "\n",
        "def load_and_split():\n",
        "  df_prom = pd.read_csv(os.path.join(base_dir, 'promotional.csv'))\n",
        "  df_neutral = pd.read_csv(os.path.join(base_dir, 'good.csv'))\n",
        "  df_prom = pd.DataFrame(df_prom['text'])\n",
        "  df_prom['label'] = 1\n",
        "  df_neutral = pd.DataFrame(df_neutral['text'])\n",
        "  df_neutral['label'] = 0\n",
        "  df = pd.concat([df_prom, df_neutral])\n",
        "\n",
        "  df_train, df_val, df_test = split_3(df)\n",
        "  df_train.to_csv(os.path.join(base_dir, 'train.csv'))\n",
        "  df_val.to_csv(os.path.join(base_dir, 'val.csv'))\n",
        "  df_test.to_csv(os.path.join(base_dir, 'test.csv'))\n",
        "\n",
        "#load_and_split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wipHFpLaQMBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(os.path.join(base_dir, 'train.csv'))\n",
        "df_val = pd.read_csv(os.path.join(base_dir, 'val.csv'))\n",
        "df_test = pd.read_csv(os.path.join(base_dir, 'test.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWv_xbZNRram",
        "colab_type": "code",
        "outputId": "d3403d7e-2095-4820-b7da-c3778ad231b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install unidecode -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjTN_yCeR9pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from tqdm.notebook import tqdm\n",
        "from unidecode import unidecode\n",
        "from textblob import TextBlob\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "def process_text(text):\n",
        "  # join individual posts\n",
        "  raw = ' '.join(text.split('|||'))\n",
        "  \n",
        "  # unicode to ascii\n",
        "  result = unidecode(raw)\n",
        "\n",
        "  # remove anything that's not a letter\n",
        "  result = re.sub(r\"[^a-zA-Z]\", \" \", result)\n",
        "\n",
        "  # remove any consecutive spaces\n",
        "  result = re.sub(' +', ' ', result)\n",
        "\n",
        "  # lowercase\n",
        "  result = result.lower()\n",
        "\n",
        "  # remove stop words\n",
        "  results = ' '.join([w for w in result.split(' ') if w not in cachedStopWords])\n",
        "\n",
        "  # stem\n",
        "  result = stemmer.stem(results)\n",
        "\n",
        "  # lemmatise\n",
        "  result = ' '.join([lemmatiser.lemmatize(w) for w in result.split(' ')])\n",
        "\n",
        "  # remove beginning and end spaces\n",
        "  result = result.strip()\n",
        "\n",
        "  return result\n",
        "\n",
        "def process_df(df):\n",
        "  X = []\n",
        "  y = []\n",
        "  for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text = row['text']\n",
        "    processed = process_text(text)\n",
        "\n",
        "    #print(processed)\n",
        "\n",
        "    X.append(processed)\n",
        "    y.append(row['label'])\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf3z88zeWE7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def preprocess_and_dump():\n",
        "  X_train_txt, y_train = process_df(df_train)\n",
        "  X_val_txt, y_val = process_df(df_val)\n",
        "\n",
        "  with open(os.path.join(base_dir, 'preprocessed.pkl'), 'wb') as file_out:\n",
        "    pickle.dump({'X_train': X_train_txt, 'y_train': y_train, 'X_val':X_val_txt, 'y_val':y_val}, file_out)\n",
        "\n",
        "#preprocess_and_dump()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TW7v-VHyM",
        "colab_type": "code",
        "outputId": "478728eb-bee2-4997-b5a7-78c474bc5a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "51aee86b3f764d158dea827bbcb2eac1",
            "28ac25f70788475893fac01a0dffe496",
            "0a4879e1b93d49d9b1f7ae9617629d26",
            "a68f8f0e981d47ba84e3b7a29ca21aa8",
            "9c9ea777de204612ad891261d7af8453",
            "67fbdb87c4214e949e6026416eb8d54a",
            "cc1bc3a46a7e45aabd84da2ab62a93ad",
            "270bc8f728e14353ac4fe8647eb003c4"
          ]
        }
      },
      "source": [
        "X_test, y_test = process_df(df_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51aee86b3f764d158dea827bbcb2eac1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10824.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T41hqytSRXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_preprocess_dump():\n",
        "  with open(os.path.join(base_dir, 'preprocessed.pkl'), 'rb') as file_in:\n",
        "    data = pickle.load(file_in)  \n",
        "  return data['X_train'], data['y_train'], data['X_val'], data['y_val']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRiHAN9EVklj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_txt, y_train, X_val_txt, y_val = load_preprocess_dump()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9ltWCeU85z",
        "colab_type": "code",
        "outputId": "eac61775-12d2-4c6e-ba19-552695377f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 400\n",
        "\n",
        "text_data = [str(txt) for txt in X_train_txt]\n",
        "val_text_data = [str(txt) for txt in X_val_txt]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(text_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf-XwE8EVsru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text_data = [str(txt) for txt in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QwBMtP9VGHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(text_data)\n",
        "X_val = tokenizer.texts_to_sequences(val_text_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDt0VKmwVtmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_text_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMPUvbSVG8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNA4fAa8VxDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFiNvi5cVNfL",
        "colab_type": "code",
        "outputId": "5c68442d-c2a7-4b29-8122-6187671c6a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "embedding_dir = '/content/gdrive/My Drive/AP_Compsci_Myers_Briggs/gensim'\n",
        "\n",
        "EMBEDDING_FILES = [os.path.join(embedding_dir, 'crawl-300d-2M.gensim'),\n",
        "                   os.path.join(embedding_dir, 'glove.840B.300d.gensim')]\n",
        "\n",
        "def build_matrix(word_index, path): # https://www.kaggle.com/thousandvoices/simple-lstm/code\n",
        "    embedding_index = KeyedVectors.load(path, mmap='r')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_index.vectors.shape[1]))\n",
        "    for word, i in word_index.items():\n",
        "        for candidate in [word, word.lower()]:\n",
        "            if candidate in embedding_index:\n",
        "                embedding_matrix[i] = embedding_index[candidate]\n",
        "                break\n",
        "    return embedding_matrix\n",
        "\n",
        "matrices = []\n",
        "\n",
        "for f in EMBEDDING_FILES:\n",
        "  print(f)\n",
        "  matrices.append(build_matrix(tokenizer.word_index, f))\n",
        "\n",
        "embedding_matrix = np.concatenate(matrices, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/AP_Compsci_Myers_Briggs/gensim/crawl-300d-2M.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/AP_Compsci_Myers_Briggs/gensim/glove.840B.300d.gensim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsEb63-pXrnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional, SpatialDropout1D, GRU, Concatenate, Input, GlobalAveragePooling1D, GlobalMaxPool1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.initializers import Constant\n",
        "\n",
        "reg = 0\n",
        "\n",
        "text_in = Input((MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "embed = Embedding(MAX_NB_WORDS,\n",
        "                    embedding_matrix.shape[1],\n",
        "                    weights=[embedding_matrix[:MAX_NB_WORDS]],\n",
        "                    trainable=False)(text_in)\n",
        "\n",
        "embed = SpatialDropout1D(0.5)(embed)\n",
        "\n",
        "lstm_out = Bidirectional(LSTM(40,\n",
        "                          kernel_regularizer=l2(reg),\n",
        "                          recurrent_regularizer=l2(reg),\n",
        "                          bias_regularizer=l2(reg)))(embed)\n",
        "\n",
        "#last_state = Concatenate(axis=-1)([gru_h1, gru_h2])\n",
        "\n",
        "#concat = GlobalAveragePooling1D()(gru_out)\n",
        "\n",
        "out = Dense(1, activation='sigmoid')(lstm_out)\n",
        "\n",
        "model = Model(inputs=[text_in], outputs=[out])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(1e-2), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6lXTgXviEjR",
        "colab_type": "code",
        "outputId": "e4c9d79c-1dab-4130-e50e-ee6bc9d54841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 400, 600)          12000000  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 400, 600)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 80)                205120    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 81        \n",
            "=================================================================\n",
            "Total params: 12,205,201\n",
            "Trainable params: 205,201\n",
            "Non-trainable params: 12,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGSCHwOjdX0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_learning_rate():\n",
        "  try:\n",
        "    from keras_lr_finder import LRFinder\n",
        "  except ModuleNotFoundError:\n",
        "    print('Installing Keras LRFinder...')\n",
        "    !pip install -q git+https://github.com/surmenok/keras_lr_finder.git\n",
        "    from keras_lr_finder import LRFinder\n",
        "\n",
        "  lr_finder = LRFinder(model)\n",
        "  lr_finder.find(X_train, y_train, start_lr=0.00001, end_lr=100, batch_size=512, epochs=10)\n",
        "\n",
        "  lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n",
        "\n",
        "#find_learning_rate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MePuvkCaoLa",
        "colab_type": "code",
        "outputId": "abe3857c-b16d-47f1-b052-950946d152dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=512,\n",
        "                    validation_data=[X_val, y_val])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 37881 samples, validate on 5411 samples\n",
            "Epoch 1/10\n",
            "37881/37881 [==============================] - 109s 3ms/step - loss: 0.2849 - accuracy: 0.8832 - val_loss: 0.1989 - val_accuracy: 0.9272\n",
            "Epoch 2/10\n",
            "37881/37881 [==============================] - 113s 3ms/step - loss: 0.1970 - accuracy: 0.9240 - val_loss: 0.1708 - val_accuracy: 0.9340\n",
            "Epoch 3/10\n",
            "37881/37881 [==============================] - 111s 3ms/step - loss: 0.1707 - accuracy: 0.9335 - val_loss: 0.1648 - val_accuracy: 0.9386\n",
            "Epoch 4/10\n",
            "37881/37881 [==============================] - 114s 3ms/step - loss: 0.1591 - accuracy: 0.9395 - val_loss: 0.1493 - val_accuracy: 0.9412\n",
            "Epoch 5/10\n",
            "37881/37881 [==============================] - 107s 3ms/step - loss: 0.1490 - accuracy: 0.9411 - val_loss: 0.1518 - val_accuracy: 0.9425\n",
            "Epoch 6/10\n",
            "33280/37881 [=========================>....] - ETA: 9s - loss: 0.1465 - accuracy: 0.9426 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aca21ae91cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=[X_val, y_val])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_4OfmHNUQTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(os.path.join(base_dir, 'model_%.2f_%.2f.h5' % (0.1518, 0.9425)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKw6HkT2U1aC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(base_dir, 'tokenizer_0.1518_0.9425.pkl'), 'wb') as tkn_out:\n",
        "  pickle.dump(tokenizer, tkn_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfuAlw9YVgZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuZl7q9Txkhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(X_test, batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKooqvXXP5s",
        "colab_type": "code",
        "outputId": "36fb573e-cea6-427b-8a7e-08953e2114a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4f30429c224a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JioEmK7qV42D",
        "colab_type": "code",
        "outputId": "20c97a4b-2cc8-4b7d-8277-97f6402fe69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(np.array(y_test), preds[:,0].round())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9318352059925094"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2mIBwUMYqRH",
        "colab_type": "code",
        "outputId": "fd978560-a80f-428e-ccff-cf4c4a6fbee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(np.array(y_test), preds[:,0].round())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411492978566149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tms1Mxcym1v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('tokenizer_0.1496.pkl', 'wb') as tkn_out:\n",
        "  pickle.dump(tokenizer, tkn_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-_qjK4knsor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_0.1496.h5')\n",
        "from tensorflow.keras.models import load_model\n",
        "model_tf = load_model('model_0.1496.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svfKO8ksnPTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_tf)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_path = 'model_0.1496.tflite'\n",
        "\n",
        "print(tflite_model_path)\n",
        "\n",
        "with tf.io.gfile.GFile(tflite_model_path, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}